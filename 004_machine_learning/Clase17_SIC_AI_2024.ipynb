{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging, Random Forest y Boosting\n",
    "\n",
    "## 1. Introducción\n",
    "\n",
    "En el aprendizaje automático, los métodos de ensamble son técnicas poderosas que combinan múltiples modelos para mejorar la precisión y la robustez de las predicciones. En esta clase, nos enfocaremos en dos de las técnicas de ensamble más comunes: **Bagging** (Bootstrap Aggregating) y **Boosting**. Además, veremos cómo el **Random Forest** se construye a partir del concepto de Bagging aplicado a árboles de decisión.\n",
    "\n",
    "## 2. Bagging\n",
    "\n",
    "### 2.1 ¿Qué es Bagging?\n",
    "\n",
    "**Bagging** es una técnica que mejora la estabilidad y precisión de los algoritmos de aprendizaje de máquina al reducir la variabilidad. Se basa en el uso de muestras bootstrap, donde se generan subconjuntos de datos a partir del conjunto de entrenamiento original mediante muestreo con reemplazo. Cada subconjunto se utiliza para entrenar un modelo individual, y las predicciones finales se obtienen mediante votación (en clasificación) o promediando las salidas (en regresión).\n",
    "\n",
    "### 2.2 Pasos de Bagging\n",
    "\n",
    "1. **Generación de Bootstrap Samples**: A partir del conjunto de datos de entrenamiento, se generan múltiples subconjuntos mediante muestreo con reemplazo.\n",
    "2. **Entrenamiento de Modelos**: Cada subconjunto se utiliza para entrenar un modelo independiente (por ejemplo, un árbol de decisión).\n",
    "3. **Combinación de Predicciones**: Para la clasificación, las predicciones se combinan utilizando la votación mayoritaria. Para la regresión, se promedian las predicciones.\n",
    "\n",
    "### 2.3 Ventajas y Desventajas\n",
    "\n",
    "- **Ventajas**:\n",
    "  - Reduce el sobreajuste (overfitting) al combinar modelos.\n",
    "  - Fácil de implementar.\n",
    "- **Desventajas**:\n",
    "  - Aumenta el costo computacional.\n",
    "  - No siempre mejora la precisión.\n",
    "\n",
    "## 3. Random Forest\n",
    "\n",
    "### 3.1 ¿Qué es Random Forest?\n",
    "\n",
    "**Random Forest** es una extensión del Bagging que utiliza árboles de decisión como modelos base. La clave de Random Forest es la introducción de aleatoriedad adicional al seleccionar subconjuntos aleatorios de características en cada división de los árboles.\n",
    "\n",
    "### 3.2 Funcionamiento de Random Forest\n",
    "\n",
    "1. **Generación de Subconjuntos de Datos**: Igual que en Bagging, se crean subconjuntos bootstrap del conjunto de datos de entrenamiento.\n",
    "2. **Selección Aleatoria de Características**: En cada nodo de cada árbol, se selecciona un subconjunto aleatorio de características para determinar la mejor división.\n",
    "3. **Entrenamiento de Árboles**: Los árboles se entrenan sobre los subconjuntos de datos y características seleccionados.\n",
    "4. **Combinación de Predicciones**: Al igual que en Bagging, se utiliza la votación mayoritaria o el promedio para obtener la predicción final.\n",
    "\n",
    "### 3.3 Ventajas y Desventajas\n",
    "\n",
    "- **Ventajas**:\n",
    "  - Mejora la precisión en comparación con un solo árbol de decisión.\n",
    "  - Maneja bien conjuntos de datos con muchas características.\n",
    "  - Reduce la varianza del modelo.\n",
    "- **Desventajas**:\n",
    "  - Menos interpretabilidad debido a la combinación de múltiples árboles.\n",
    "  - Mayor costo computacional.\n",
    "\n",
    "## 4. Boosting\n",
    "\n",
    "### 4.1 ¿Qué es Boosting?\n",
    "\n",
    "**Boosting** es otra técnica de ensamble, pero a diferencia de Bagging, se enfoca en construir modelos de manera secuencial, donde cada modelo nuevo intenta corregir los errores cometidos por los modelos anteriores. Uno de los algoritmos más conocidos de Boosting es **AdaBoost**.\n",
    "\n",
    "### 4.2 Funcionamiento de Boosting\n",
    "\n",
    "1. **Inicialización**: Se asignan pesos iguales a todas las observaciones del conjunto de datos.\n",
    "2. **Entrenamiento Secuencial**: Se entrena un modelo y se calcula el error de predicción. Los pesos de las observaciones incorrectamente clasificadas se incrementan.\n",
    "3. **Ajuste de Pesos**: Se entrenan modelos adicionales, donde cada uno se enfoca en corregir los errores de los modelos previos.\n",
    "4. **Combinación de Modelos**: Las predicciones de los modelos se combinan de manera ponderada para formar la predicción final.\n",
    "\n",
    "### 4.3 Ventajas y Desventajas\n",
    "\n",
    "- **Ventajas**:\n",
    "  - Alta precisión.\n",
    "  - Eficaz para corregir errores de clasificación.\n",
    "- **Desventajas**:\n",
    "  - Sensible al ruido en los datos.\n",
    "  - Requiere más tiempo de entrenamiento.\n",
    "\n",
    "## 5. Comparación entre Bagging y Boosting\n",
    "\n",
    "| **Aspecto**       | **Bagging**                               | **Boosting**                               |\n",
    "|-------------------|-------------------------------------------|--------------------------------------------|\n",
    "| **Entrenamiento** | Paralelo, independiente entre modelos     | Secuencial, cada modelo depende del anterior |\n",
    "| **Enfoque**       | Reduce la varianza                        | Reduce el sesgo                            |\n",
    "| **Sensibilidad al Ruido** | Menos sensible                      | Más sensible                               |\n",
    "| **Combinación**   | Votación/Promedio simple                  | Combinación ponderada                      |\n",
    "\n",
    "## 6. Conclusiones\n",
    "\n",
    "El uso de métodos de ensamble como Bagging, Random Forest, y Boosting permite construir modelos más robustos y precisos. Mientras que Bagging y Random Forest se enfocan en reducir la varianza y evitar el sobreajuste, Boosting busca mejorar el rendimiento corrigiendo los errores de predicción.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
