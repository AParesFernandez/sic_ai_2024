{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2024/blob/main/007_depp_learning/Clase30_SIC_AI_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmInPWbRFGQU"
      },
      "source": [
        "# Redes Neuronales Avanzadas: RNN, LSTM, Autoencoder y GAN\n",
        "\n",
        "Este notebook cubre los siguientes temas:\n",
        "1. Redes Neuronales Recurrentes (RNN)\n",
        "2. Memoria a Corto y Largo Plazo (LSTM)\n",
        "3. Autoencoders\n",
        "4. Redes Generativas Adversarias (GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pdxaqtnFGQW"
      },
      "source": [
        "## 1. Redes Neuronales Recurrentes (RNN)\n",
        "\n",
        "Las RNN son un tipo de red neuronal diseñada para trabajar con datos secuenciales."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las Redes Neuronales Recurrentes (RNN) son un tipo de arquitectura de red neuronal diseñada específicamente para procesar datos secuenciales. A diferencia de las redes neuronales feedforward tradicionales, las RNN tienen conexiones que forman ciclos, permitiéndoles mantener una especie de \"memoria\" de entradas anteriores.\n",
        "##Características clave de las RNN:\n",
        "\n",
        "1. Procesamiento secuencial: Las RNN procesan datos de entrada uno a la vez en una secuencia, lo que las hace ideales para tareas como procesamiento de lenguaje natural o análisis de series temporales.\n",
        "\n",
        "2. Estado oculto: Mantienen un estado interno (llamado estado oculto) que se actualiza con cada nueva entrada, permitiendo a la red \"recordar\" información de pasos anteriores.\n",
        "Compartición de parámetros: Las RNN utilizan los mismos parámetros para cada paso de la secuencia, lo que reduce el número total de parámetros que necesitan ser aprendidos.\n",
        "\n",
        "3. Capacidad de manejar secuencias de longitud variable: Pueden procesar secuencias de diferentes longitudes, lo que es útil en muchas aplicaciones del mundo real.\n",
        "\n",
        "\n",
        "Sin embargo, las RNN básicas tienen dificultades para capturar dependencias a largo plazo debido al problema del desvanecimiento del gradiente, lo que llevó al desarrollo de arquitecturas más avanzadas como LSTM."
      ],
      "metadata": {
        "id": "nshzT-JVFiYp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfoSCuGkFGQW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ejemplo simple de RNN\n",
        "rnn = tf.keras.layers.SimpleRNN(units=64, input_shape=(None, 1))\n",
        "model = tf.keras.Sequential([rnn, tf.keras.layers.Dense(1)])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKSAR065FGQX"
      },
      "source": [
        "## 2. Memoria a Corto y Largo Plazo (LSTM)\n",
        "\n",
        "LSTM es una variante de RNN que puede aprender dependencias a largo plazo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las redes de Memoria a Corto y Largo Plazo (LSTM) son una variante especializada de RNN diseñada para superar el problema del desvanecimiento del gradiente y capturar mejor las dependencias a largo plazo en los datos secuenciales.\n",
        "##Características clave de las LSTM:\n",
        "\n",
        "1. Estructura de celda compleja: Las LSTM utilizan una estructura de celda más compleja que incluye varias \"puertas\" (gates) para controlar el flujo de información.\n",
        "\n",
        "2. Puerta de olvido: Decide qué información del estado anterior debe ser descartada.\n",
        "\n",
        "3. Puerta de entrada: Determina qué nueva información se almacenará en el estado de la celda.\n",
        "\n",
        "4. Puerta de salida: Controla qué partes del estado de la celda se outputearán.\n",
        "5. Línea de memoria a largo plazo: Permite que la información fluya a través de muchos pasos de tiempo con cambios mínimos, ayudando a capturar dependencias a largo plazo.\n",
        "\n",
        "Las LSTM son particularmente efectivas en tareas que requieren memoria a largo plazo, como la traducción automática, el reconocimiento de voz y la generación de texto."
      ],
      "metadata": {
        "id": "PKromTZWFzqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6bwGHRbFGQX"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de LSTM\n",
        "lstm = tf.keras.layers.LSTM(units=64, input_shape=(None, 1))\n",
        "model_lstm = tf.keras.Sequential([lstm, tf.keras.layers.Dense(1)])\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "print(model_lstm.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwZCoGdXFGQY"
      },
      "source": [
        "## 3. Autoencoders\n",
        "\n",
        "Los autoencoders son redes neuronales utilizadas para aprender representaciones eficientes de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los Autoencoders son un tipo de red neuronal no supervisada diseñada para aprender representaciones eficientes (codificaciones) de los datos de entrada, típicamente para reducción de dimensionalidad o para aprendizaje de características.\n",
        "##Características clave de los Autoencoders:\n",
        "\n",
        "1. Arquitectura simétrica: Consisten en un codificador que comprime los datos de entrada y un decodificador que intenta reconstruir los datos originales a partir de la representación comprimida.\n",
        "\n",
        "2. Cuello de botella: La capa central (capa latente) generalmente tiene menos dimensiones que la entrada, forzando a la red a aprender una representación comprimida de los datos.\n",
        "\n",
        "3. Aprendizaje no supervisado: No requieren etiquetas para el entrenamiento, ya que el objetivo es reconstruir la entrada.\n",
        "\n",
        "4. Variantes especializadas: Existen varios tipos de autoencoders, como los Autoencoders Variacionales (VAE) o los Autoencoders de Eliminación de Ruido (DAE), cada uno con propósitos específicos.\n",
        "\n",
        "Los Autoencoders se utilizan en una variedad de aplicaciones, incluyendo reducción de ruido, detección de anomalías y generación de características para otros modelos de aprendizaje automático."
      ],
      "metadata": {
        "id": "WlmUT8RmGEIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMjhCFU6FGQY"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de Autoencoder\n",
        "input_dim = 784  # para MNIST\n",
        "encoding_dim = 32\n",
        "\n",
        "input_img = tf.keras.Input(shape=(input_dim,))\n",
        "encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "decoded = tf.keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = tf.keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "print(autoencoder.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foZf5owXFGQY"
      },
      "source": [
        "## 4. Redes Generativas Adversarias (GAN)\n",
        "\n",
        "Las GAN consisten en dos redes que compiten entre sí: un generador y un discriminador."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las Redes Generativas Adversarias (GAN) son un marco de aprendizaje no supervisado que consiste en dos redes neuronales que compiten entre sí: un generador y un discriminador.\n",
        "##Características clave de las GAN:\n",
        "\n",
        "1. Generador: Intenta crear datos sintéticos que sean indistinguibles de los datos reales.\n",
        "\n",
        "2. Discriminador: Intenta distinguir entre los datos reales y los generados sintéticamente.\n",
        "\n",
        "3. Entrenamiento adversario: Las dos redes se entrenan simultáneamente, con el generador tratando de engañar al discriminador y el discriminador tratando de no ser engañado.\n",
        "\n",
        "4. Aprendizaje no supervisado: No requieren etiquetas para el entrenamiento, ya que aprenden de la distribución de los datos de entrada.\n",
        "\n",
        "5. Versatilidad: Pueden ser aplicadas a una amplia gama de tareas, incluyendo generación de imágenes, traducción de imagen a imagen, y síntesis de texto.\n",
        "\n",
        "Las GAN han mostrado resultados impresionantes en la generación de imágenes realistas, la mejora de la resolución de imágenes, y la creación de arte, entre otras aplicaciones. Sin embargo, su entrenamiento puede ser desafiante debido a la necesidad de equilibrar el rendimiento del generador y el discriminador.\n",
        "\n",
        "\n",
        "Estas arquitecturas avanzadas de redes neuronales han expandido significativamente las capacidades del aprendizaje profundo, permitiendo abordar tareas cada vez más complejas en procesamiento de secuencias, comprensión y generación de datos."
      ],
      "metadata": {
        "id": "XKA0wyiFGQFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6epjLAdFGQY"
      },
      "outputs": [],
      "source": [
        "# Ejemplo simple de GAN\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Reshape((7, 7, 256)),\n",
        "        tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "print(\"Generador:\")\n",
        "print(generator.summary())\n",
        "print(\"\\nDiscriminador:\")\n",
        "print(discriminator.summary())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}