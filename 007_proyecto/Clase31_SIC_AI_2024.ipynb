{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de recomendación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Qué son los sistemas de recomendación?\n",
    "\n",
    "Los sistemas de recomendación son algoritmos diseñados para sugerir elementos (productos, películas, música, etc.) a los usuarios en función de sus preferencias y comportamientos anteriores. Estos sistemas se han vuelto fundamentales en diversas aplicaciones modernas, como tiendas en línea, plataformas de streaming y redes sociales.\n",
    "\n",
    "### Tipos de Sistemas de Recomendación\n",
    "\n",
    "1. **Filtrado colaborativo basado en usuarios**: \n",
    "   Este enfoque recomienda productos a un usuario basándose en las preferencias de usuarios similares. La idea es que si un grupo de usuarios tiene gustos parecidos, los elementos que un usuario ha valorado positivamente pueden ser recomendados a otros usuarios similares.\n",
    "\n",
    "2. **Filtrado colaborativo basado en ítems**: \n",
    "   En este enfoque, las recomendaciones se hacen basadas en la similitud entre productos. Si un usuario ha valorado positivamente un ítem, se le recomendarán ítems similares.\n",
    "\n",
    "3. **Modelos basados en contenido**: \n",
    "   Se recomiendan ítems similares a aquellos que un usuario ya ha mostrado interés, basándose en las características de los ítems, como género, autor, director, etc.\n",
    "\n",
    "4. **Sistemas híbridos**: \n",
    "   Combinan diferentes enfoques, como el filtrado colaborativo y el contenido, para mejorar la precisión de las recomendaciones.\n",
    "\n",
    "## ¿Por qué son importantes?\n",
    "\n",
    "Los sistemas de recomendación son cruciales porque mejoran la experiencia del usuario al reducir la sobrecarga de información y personalizar la oferta de contenido o productos. Además, ayudan a las empresas a aumentar sus ventas o retener a los usuarios al ofrecer recomendaciones más precisas y relevantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO SISTEMA DE RECOMENDACIÓN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset de MovieLens\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(url, sep='\\t', names=columns)\n",
    "\n",
    "# Preprocesar los datos\n",
    "data = data.drop('timestamp', axis=1)\n",
    "\n",
    "# Crear la matriz usuario-item\n",
    "user_item_matrix = data.pivot_table(index='user_id', columns='item_id', values='rating')\n",
    "\n",
    "# Rellenar los valores NaN con ceros (puedes optar por otra estrategia, como promedios)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "\n",
    "# Calcular la similitud entre usuarios utilizando la similitud del coseno\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "\n",
    "# Convertir la matriz de similitud en un DataFrame para facilitar el manejo\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "\n",
    "# Función para hacer recomendaciones basadas en la similitud de usuarios\n",
    "def recommend_movies(user_id, n_recommendations=5):\n",
    "    # Obtener la similitud del usuario dado con otros usuarios\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)\n",
    "\n",
    "    # Obtener las películas que el usuario aún no ha calificado\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    unrated_movies = user_ratings[user_ratings == 0].index\n",
    "\n",
    "    # Hacer la media ponderada de las calificaciones de los usuarios similares para las películas no calificadas\n",
    "    recommendations = {}\n",
    "    for movie in unrated_movies:\n",
    "        similar_users_ratings = user_item_matrix.loc[similar_users.index, movie]\n",
    "        weighted_sum = np.dot(similar_users.values, similar_users_ratings.values)\n",
    "        recommendations[movie] = weighted_sum\n",
    "\n",
    "    # Ordenar las recomendaciones y devolver las mejores\n",
    "    recommended_movies = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    \n",
    "    return recommended_movies\n",
    "\n",
    "# Ejemplo: Hacer recomendaciones para el usuario con ID 1\n",
    "recommendations = recommend_movies(1)\n",
    "print(f\"Recomendaciones para el usuario 1: {recommendations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bases de Datos para Practicar Sistemas de Recomendación\n",
    "\n",
    "A continuación, te dejo una lista de bases de datos públicas que puedes utilizar para practicar y desarrollar sistemas de recomendación. Estas bases de datos cubren diferentes dominios, como películas, música, libros y productos.\n",
    "\n",
    "## 1. [MovieLens](https://grouplens.org/datasets/movielens/)\n",
    "**Descripción**: \n",
    "- MovieLens es una de las bases de datos más populares para sistemas de recomendación. Contiene calificaciones de películas realizadas por usuarios.\n",
    "- Disponible en diferentes tamaños:\n",
    "  - **100k**: 100,000 calificaciones por 943 usuarios sobre 1,682 películas.\n",
    "  - **1M**: 1 millón de calificaciones por 6,000 usuarios sobre 4,000 películas.\n",
    "  - **20M**: 20 millones de calificaciones de películas.\n",
    "\n",
    "**Aplicación**: Recomendación de películas.\n",
    "\n",
    "## 2. [Jester](http://eigentaste.berkeley.edu/dataset/)\n",
    "**Descripción**: \n",
    "- Este dataset contiene calificaciones de 100 chistes por parte de más de 73,000 usuarios.\n",
    "- Ideal para sistemas de recomendación de entretenimiento o basados en preferencias humorísticas.\n",
    "\n",
    "**Aplicación**: Recomendación de chistes o contenido humorístico.\n",
    "\n",
    "## 3. [Amazon Product Data](http://jmcauley.ucsd.edu/data/amazon/)\n",
    "**Descripción**: \n",
    "- Contiene reseñas de productos en Amazon, organizadas por categoría (libros, electrónica, ropa, etc.).\n",
    "- Incluye información sobre calificaciones, texto de las reseñas y relaciones entre productos.\n",
    "\n",
    "**Aplicación**: Recomendación de productos, análisis de sentimientos, o sistemas basados en reseñas.\n",
    "\n",
    "## 4. [Book-Crossing Dataset](https://www.kaggle.com/datasets/somnambwl/bookcrossing-dataset)\n",
    "**Descripción**: \n",
    "- Dataset de recomendaciones de libros que incluye 1.1 millones de calificaciones de 278,000 usuarios sobre 271,000 libros.\n",
    "- Incluye información sobre los usuarios, como su edad y ubicación.\n",
    "\n",
    "**Aplicación**: Recomendación de libros.\n",
    "\n",
    "## 5. [Last.fm Dataset](http://millionsongdataset.com/lastfm/)\n",
    "**Descripción**: \n",
    "- Contiene interacciones entre usuarios y música (pistas reproducidas, artistas, etc.) de la plataforma Last.fm.\n",
    "- El dataset incluye datos de 1,000 usuarios con más de 1 millón de registros de reproducciones.\n",
    "\n",
    "**Aplicación**: Recomendación de música y artistas.\n",
    "\n",
    "## 6. [Netflix Prize Dataset](https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data)\n",
    "**Descripción**: \n",
    "- Dataset utilizado para la competencia **Netflix Prize**. Contiene millones de calificaciones de usuarios sobre películas.\n",
    "- Aunque la competencia ya terminó, este dataset sigue siendo muy útil para la investigación en recomendaciones.\n",
    "\n",
    "**Aplicación**: Recomendación de películas y contenido multimedia.\n",
    "\n",
    "## 7. [Yelp Dataset](https://www.yelp.com/dataset)\n",
    "**Descripción**: \n",
    "- Contiene reseñas y calificaciones de empresas locales (restaurantes, tiendas, etc.) en diferentes ciudades.\n",
    "- Además de calificaciones, incluye información de los usuarios y las empresas, como categorías, ubicación y horarios.\n",
    "\n",
    "**Aplicación**: Recomendación de negocios y servicios locales.\n",
    "\n",
    "## 8. [Goodreads Book Reviews](https://www.kaggle.com/datasets/pypiahmad/goodreads-book-reviews1)\n",
    "**Descripción**: \n",
    "- Dataset que contiene reseñas y calificaciones de libros de **Goodreads**.\n",
    "- Se puede utilizar para recomendar libros basándose en las calificaciones y el texto de las reseñas.\n",
    "\n",
    "**Aplicación**: Recomendación de libros y análisis de reseñas.\n",
    "\n",
    "## 9. [Spotify Million Playlist Dataset](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge)\n",
    "**Descripción**: \n",
    "- Dataset de Spotify que contiene 1 millón de listas de reproducción creadas por usuarios. Cada lista de reproducción incluye una serie de canciones seleccionadas por los usuarios.\n",
    "\n",
    "**Aplicación**: Recomendación de música y generación de listas de reproducción.\n",
    "\n",
    "## 10. [IMDb Dataset](https://www.imdb.com/interfaces/)\n",
    "**Descripción**: \n",
    "- Dataset proporcionado por IMDb, que incluye información sobre películas, programas de televisión, actores, directores, y mucho más.\n",
    "- Aunque no contiene calificaciones directas, se puede usar para sistemas de recomendación basados en contenido (género, director, etc.).\n",
    "\n",
    "**Aplicación**: Recomendación de películas y series basadas en metadatos.\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaciones adicionales\n",
    "\n",
    "### [Kaggle](https://www.kaggle.com/datasets)\n",
    "Kaggle es una excelente fuente de datasets para recomendaciones y aprendizaje automático. Puedes encontrar datasets etiquetados como **Recommender Systems** o explorar cualquier dataset para crear tus propios sistemas de recomendación.\n",
    "\n",
    "---\n",
    "\n",
    "### Consideraciones al elegir una base de datos:\n",
    "1. **Tamaño del Dataset**: Algunos datasets son muy grandes, por lo que debes asegurarte de tener suficiente capacidad de procesamiento.\n",
    "2. **Dominio**: Elige un dataset que esté alineado con el tipo de sistema de recomendación que quieras construir (películas, libros, música, productos, etc.).\n",
    "3. **Información Adicional**: Algunas bases de datos ofrecen información adicional como texto de reseñas, categorías de productos, géneros de películas, etc., lo que puede ser útil para implementar **modelos híbridos** o mejorar el rendimiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Por qué usar scikit-surprise?\n",
    "\n",
    "## ¿Qué es scikit-surprise?\n",
    "\n",
    "**`scikit-surprise`** es una biblioteca de Python diseñada específicamente para implementar y evaluar sistemas de recomendación. A diferencia de otras herramientas generales de machine learning, **surprise** está enfocada en tareas específicas de recomendación y simplifica mucho el manejo de datos de calificaciones.\n",
    "\n",
    "### Ventajas de usar `scikit-surprise`:\n",
    "\n",
    "1. **Facilidad de uso**:\n",
    "   `scikit-surprise` permite implementar algoritmos complejos de sistemas de recomendación con muy pocas líneas de código. Los algoritmos más utilizados, como **SVD** (Singular Value Decomposition) o **KNN** (K Nearest Neighbors), están ya integrados.\n",
    "\n",
    "2. **Datasets incorporados**:\n",
    "   La biblioteca proporciona acceso a datasets populares de calificaciones, como **MovieLens** o **Jester**, lo que facilita la experimentación y aprendizaje sin la necesidad de buscar y preparar datos externos.\n",
    "\n",
    "3. **Evaluación y validación**:\n",
    "   `scikit-surprise` proporciona herramientas para realizar validaciones cruzadas y evaluar el rendimiento de los modelos, lo que es clave para comparar la efectividad de diferentes enfoques de recomendación.\n",
    "\n",
    "4. **Extensibilidad**:\n",
    "   Puedes implementar tus propios algoritmos de recomendación si los existentes no cumplen con tus necesidades. `scikit-surprise` te permite definir modelos personalizados con facilidad.\n",
    "\n",
    "5. **Soporte para múltiples algoritmos**:\n",
    "   La biblioteca incluye soporte para diversos algoritmos de filtrado colaborativo, tanto basado en usuarios como en ítems, así como modelos basados en matrices factorizadas, como **SVD** y **NMF**.\n",
    "\n",
    "### Algoritmos disponibles en scikit-surprise:\n",
    "\n",
    "- **SVD**: Uno de los más utilizados, basado en la factorización de matrices.\n",
    "- **KNNBasic**: Filtrado colaborativo con vecinos más cercanos.\n",
    "- **Slope One**: Un algoritmo simple y efectivo para sistemas de recomendación.\n",
    "- **Co-Clustering**: Algoritmo que agrupa simultáneamente usuarios y elementos para recomendaciones.\n",
    "\n",
    "### Ejemplo básico:\n",
    "\n",
    "```python\n",
    "from surprise import Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Cargar el dataset MovieLens 100k directamente desde Surprise\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Usar el algoritmo SVD\n",
    "algo = SVD()\n",
    "\n",
    "# Validar el modelo con validación cruzada\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo primero es instalar\n",
    "%pip install scikit-surprise gcsfs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver \n",
    "# https://surprise.readthedocs.io/en/stable/_modules/surprise/dataset.html#Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bases de Datos en `scikit-surprise`\n",
    "\n",
    "`scikit-surprise` incluye varios datasets incorporados que puedes cargar fácilmente para practicar la creación de sistemas de recomendación. Estos datasets están centrados principalmente en calificaciones de usuarios sobre distintos tipos de ítems como películas, libros, chistes, etc.\n",
    "\n",
    "## 1. MovieLens 100k\n",
    "\n",
    "**Descripción**: \n",
    "- Dataset de 100,000 calificaciones de 943 usuarios sobre 1,682 películas. Es uno de los datasets más utilizados para sistemas de recomendación.\n",
    "\n",
    "**Tamaño**: 100,000 calificaciones\n",
    "\n",
    "**Aplicación**: Recomendación de películas.\n",
    "\n",
    "**Cómo cargarlo**:\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MovieLens 1M\n",
    "\n",
    "**Descripción**: \n",
    "- Una versión más grande del dataset de MovieLens que contiene 1 millón de calificaciones realizadas por 6,000 usuarios sobre 4,000 películas.\n",
    "\n",
    "**Tamaño**: 1,000,000 calificaciones\n",
    "\n",
    "**Aplicación**: Recomendación de películas a mayor escala.\n",
    "\n",
    "**Cómo cargarlo**:\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "data = Dataset.load_builtin('ml-1m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Jester Dataset\n",
    "\n",
    "**Descripción**: \n",
    "- Este dataset contiene calificaciones de 100 chistes por parte de más de 73,000 usuarios. Ideal para sistemas de recomendación relacionados con contenido humorístico.\n",
    "\n",
    "**Tamaño**: Más de 1.7 millones de calificaciones de chistes\n",
    "\n",
    "**Aplicación**: Recomendación de chistes o contenido humorístico.\n",
    "\n",
    "**Cómo cargarlo**:\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "data = Dataset.load_builtin('jester')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Book-Crossing Dataset\n",
    "\n",
    "**Descripción**: \n",
    "- Un dataset de recomendaciones de libros que incluye 1.1 millones de calificaciones realizadas por 278,000 usuarios sobre 271,000 libros.\n",
    "\n",
    "**Tamaño**: 1,149,780 calificaciones\n",
    "\n",
    "**Aplicación**: Recomendación de libros.\n",
    "\n",
    "**Cómo cargarlo**:\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "data = Dataset.load_builtin('bx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Netflix Prize Dataset\n",
    "\n",
    "**Descripción**: \n",
    "- Este dataset es una muestra de las calificaciones de películas utilizadas en la famosa competencia Netflix Prize, donde se buscaba mejorar las recomendaciones de películas.\n",
    "\n",
    "**Tamaño**: Desconocido (enorme)\n",
    "\n",
    "**Aplicación**: Recomendación de películas y contenido multimedia.\n",
    "\n",
    "**Cómo cargarlo**:\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "data = Dataset.load_builtin('netflix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD en `scikit-surprise`\n",
    "\n",
    "El **SVD** (Singular Value Decomposition o Descomposición en Valores Singulares) es uno de los algoritmos más utilizados para implementar **filtrado colaborativo basado en descomposición matricial**. Este enfoque predice las calificaciones que un usuario podría dar a ítems (como películas o productos) basándose en las calificaciones anteriores, tanto del usuario como de otros usuarios similares.\n",
    "\n",
    "## ¿Cómo funciona el SVD?\n",
    "\n",
    "El algoritmo SVD descompone la matriz de calificaciones en tres matrices: \n",
    "- Una matriz que representa los usuarios.\n",
    "- Una matriz que representa los ítems.\n",
    "- Una matriz diagonal que contiene los valores singulares, que indican la importancia de cada componente.\n",
    "\n",
    "Matemáticamente, si tenemos una matriz de calificaciones **R** de tamaño \\(m \\times n\\) (donde \\(m\\) son los usuarios y \\(n\\) son los ítems), el SVD descompone esa matriz de la siguiente manera:\n",
    "\n",
    "$$\n",
    "R \\approx U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **U** es una matriz \\(m \\times k\\) que representa a los usuarios en un espacio latente.\n",
    "- **\\Sigma** es una matriz diagonal \\(k \\times k\\) que contiene los valores singulares.\n",
    "- **V^T** es una matriz \\(k \\times n\\) que representa los ítems en ese mismo espacio latente.\n",
    "\n",
    "El valor de \\(k\\) es un hiperparámetro que define el número de dimensiones en el espacio latente.\n",
    "\n",
    "## SVD en `scikit-surprise`\n",
    "\n",
    "En `scikit-surprise`, el algoritmo SVD es una implementación simplificada del enfoque utilizado por el equipo ganador del **Netflix Prize**. Se basa en descomponer la matriz de calificaciones en componentes de usuario e ítem, y luego utilizar estos componentes para hacer predicciones de calificaciones faltantes.\n",
    "\n",
    "### Parámetros importantes:\n",
    "\n",
    "- **n_factors**: Número de factores latentes. Controla la complejidad del modelo.\n",
    "- **n_epochs**: Número de iteraciones del algoritmo de optimización.\n",
    "- **lr_all**: Tasa de aprendizaje utilizada en el descenso de gradiente.\n",
    "- **reg_all**: Parámetro de regularización que ayuda a controlar el sobreajuste.\n",
    "\n",
    "## Ventajas del SVD:\n",
    "\n",
    "1. **Reducción de dimensionalidad**: Permite reducir el número de características a unas pocas dimensiones latentes, mejorando la generalización del modelo.\n",
    "  \n",
    "2. **Predicciones robustas**: SVD es eficaz para predecir calificaciones incluso cuando las matrices de calificaciones son muy dispersas (con muchas entradas faltantes).\n",
    "\n",
    "3. **Probado en la práctica**: Fue uno de los enfoques utilizados por los ganadores del **Netflix Prize**, demostrando su efectividad en escenarios de recomendación a gran escala.\n",
    "\n",
    "### ¿Cuándo NO usar SVD?\n",
    "\n",
    "- **Datos demasiado pequeños**: Si tienes pocos datos, el modelo puede no generalizar bien, ya que el algoritmo necesita suficiente información para identificar la estructura latente subyacente.\n",
    "- **Filtrado colaborativo basado en contenido**: Si las características de los ítems (géneros de películas, autores, etc.) son importantes, el SVD puede no ser la mejor opción, ya que se basa únicamente en las interacciones usuario-ítem.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo de uso de SVD en `scikit-surprise`\n",
    "\n",
    "A continuación se muestra cómo usar el algoritmo SVD en un conjunto de datos de calificaciones de películas (MovieLens 100k) utilizando la biblioteca `scikit-surprise`:\n",
    "\n",
    "```python\n",
    "# Importar las librerías necesarias\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Cargar el dataset de MovieLens 100k\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Usar el algoritmo SVD\n",
    "algo = SVD()\n",
    "\n",
    "# Entrenar el modelo en el set de entrenamiento\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predecir las calificaciones en el set de prueba\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluar el rendimiento con el error RMSE\n",
    "accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO DE EJERCICIO\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset MovieLens-100k\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# Crear el algoritmo SVD\n",
    "svd = SVD()\n",
    "\n",
    "# Entrenar el algoritmo con el conjunto de entrenamiento\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "# Calcular el RMSE (Root Mean Squared Error)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Hacer algunas recomendaciones\n",
    "def get_top_n(predictions, n=10):\n",
    "    top_n = {}\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        if uid not in top_n:\n",
    "            top_n[uid] = []\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Ordenar las predicciones para cada usuario y obtener las top n\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "top_n = get_top_n(predictions, n=5)\n",
    "\n",
    "# Imprimir las top 5 recomendaciones para el usuario 1\n",
    "print(\"\\nTop 5 recomendaciones para el usuario 1:\")\n",
    "for iid, rating in top_n['1']:\n",
    "    print(f\"Película {iid}: Calificación estimada {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNNBasic en Surprise\n",
    "\n",
    "## Introducción\n",
    "\n",
    "`KNNBasic` es un algoritmo de recomendación basado en la técnica de K-Nearest Neighbors (KNN). Este enfoque se utiliza comúnmente para sistemas de recomendación que buscan encontrar elementos similares en función de las calificaciones dadas por los usuarios. `KNNBasic` en la biblioteca Surprise permite crear recomendaciones mediante el uso de distancias entre usuarios o ítems.\n",
    "\n",
    "## ¿Cómo funciona?\n",
    "\n",
    "El algoritmo de KNN funciona identificando los **K** vecinos más cercanos de un usuario o ítem dado. Hay dos maneras principales de aplicar KNN:\n",
    "\n",
    "1. **Filtrado colaborativo basado en usuarios**: Recomendaciones basadas en usuarios similares.\n",
    "2. **Filtrado colaborativo basado en ítems**: Recomendaciones basadas en ítems similares.\n",
    "\n",
    "### Pasos Generales\n",
    "\n",
    "1. **Cargar los datos**: Utilizar un conjunto de datos que contenga interacciones entre usuarios y productos.\n",
    "2. **Crear el modelo KNN**: Especificar la métrica de similitud y el número de vecinos a considerar.\n",
    "3. **Entrenar el modelo**: Ajustar el modelo a los datos.\n",
    "4. **Hacer recomendaciones**: Predecir las calificaciones para elementos no calificados por el usuario.\n",
    "\n",
    "## Ejemplo de Código\n",
    "\n",
    "A continuación se presenta un ejemplo básico que utiliza `KNNBasic` de la biblioteca Surprise para hacer recomendaciones basadas en usuarios:\n",
    "\n",
    "```python\n",
    "# Importar las bibliotecas necesarias\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Cargar el conjunto de datos de MovieLens\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "\n",
    "# Cargar los datos\n",
    "data = Dataset.load_from_file('ml-100k/u.data', reader=reader)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Crear el modelo KNN con similaridad basada en coseno\n",
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': True  # Utilizar similitud entre usuarios\n",
    "}\n",
    "knn = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# Entrenar el modelo\n",
    "knn.fit(trainset)\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = knn.test(testset)\n",
    "\n",
    "# Evaluar el modelo\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Cargar el dataset MovieLens-100k\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# Crear el algoritmo KNNBasic\n",
    "# Usamos similitud basada en coseno y k=40 vecinos\n",
    "modelo = KNNBasic(k=40, sim_options={'name': 'cosine', 'user_based': True})\n",
    "\n",
    "# Entrenar el algoritmo con el conjunto de entrenamiento\n",
    "modelo.fit(trainset)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = modelo.test(testset)\n",
    "\n",
    "# Calcular el RMSE (Root Mean Squared Error)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Mostrar algunas predicciones individuales\n",
    "print(\"\\nAlgunas predicciones individuales:\")\n",
    "for i, prediction in enumerate(predictions[:5]):\n",
    "    print(f\"Usuario: {prediction.uid}, Película: {prediction.iid}, \"\n",
    "          f\"Calificación real: {prediction.r_ui}, Calificación estimada: {prediction.est:.2f}\")\n",
    "\n",
    "# Función para obtener las top N recomendaciones\n",
    "def get_top_n(predictions, n=5):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Ordenar las predicciones para cada usuario y obtener las top n\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return dict(top_n)\n",
    "\n",
    "# Obtener las top 5 recomendaciones para todos los usuarios\n",
    "top_n = get_top_n(predictions, n=5)\n",
    "\n",
    "# Mostrar las top 5 recomendaciones para el usuario 1\n",
    "print(\"\\nTop 5 recomendaciones para el usuario 1:\")\n",
    "for iid, rating in top_n['1']:\n",
    "    print(f\"Película {iid}: Calificación estimada {rating:.2f}\")\n",
    "\n",
    "# Mostrar todas las predicciones para el usuario 1\n",
    "print(\"\\nTodas las predicciones para el usuario 1:\")\n",
    "user_1_predictions = [pred for pred in predictions if pred.uid == '1']\n",
    "for prediction in sorted(user_1_predictions, key=lambda x: x.est, reverse=True)[:10]:\n",
    "    print(f\"Película {prediction.iid}: Calificación estimada {prediction.est:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo con nombre de películas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from surprise import KNNBasic, Dataset, Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Función para descargar y extraer el dataset MovieLens-100k\n",
    "def download_movielens_100k():\n",
    "    url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "    zip_path = \"ml-100k.zip\"\n",
    "    extract_path = \".\"\n",
    "    \n",
    "    if not os.path.exists(\"ml-100k\"):\n",
    "        print(\"Descargando MovieLens-100k dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        \n",
    "        print(\"Extrayendo archivos...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(\"Dataset descargado y extraído exitosamente.\")\n",
    "    else:\n",
    "        print(\"El dataset MovieLens-100k ya existe.\")\n",
    "\n",
    "# Descargar el dataset si no existe\n",
    "download_movielens_100k()\n",
    "\n",
    "# Cargar los metadatos de las películas\n",
    "movies_df = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1', header=None, \n",
    "                        names=['movie_id', 'title', 'release_date', 'video_release_date', \n",
    "                               'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', \n",
    "                               'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', \n",
    "                               'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', \n",
    "                               'Thriller', 'War', 'Western'])\n",
    "\n",
    "# Crear un diccionario para mapear id de película a título\n",
    "movie_id_to_title = dict(zip(movies_df['movie_id'], movies_df['title']))\n",
    "\n",
    "# Cargar el dataset de calificaciones\n",
    "file_path = os.path.expanduser('ml-100k/u.data')\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# Crear el algoritmo KNNBasic\n",
    "modelo = KNNBasic(k=40, sim_options={'name': 'cosine', 'user_based': True})\n",
    "\n",
    "# Entrenar el algoritmo con el conjunto de entrenamiento\n",
    "modelo.fit(trainset)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = modelo.test(testset)\n",
    "\n",
    "# Calcular el RMSE (Root Mean Squared Error)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Función para obtener las top N recomendaciones\n",
    "def get_top_n(predictions, n=5):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Ordenar las predicciones para cada usuario y obtener las top n\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return dict(top_n)\n",
    "\n",
    "# Obtener las top 5 recomendaciones para todos los usuarios\n",
    "top_n = get_top_n(predictions, n=5)\n",
    "\n",
    "# Mostrar las top 5 recomendaciones para el usuario 1\n",
    "print(\"\\nTop 5 recomendaciones para el usuario 1:\")\n",
    "for iid, rating in top_n['1']:\n",
    "    print(f\"Película: {movie_id_to_title[int(iid)]}, Calificación estimada: {rating:.2f}\")\n",
    "\n",
    "# Mostrar todas las predicciones para el usuario 1\n",
    "print(\"\\nTodas las predicciones para el usuario 1:\")\n",
    "user_1_predictions = [pred for pred in predictions if pred.uid == '1']\n",
    "for prediction in sorted(user_1_predictions, key=lambda x: x.est, reverse=True)[:10]:\n",
    "    print(f\"Película: {movie_id_to_title[int(prediction.iid)]}, \"\n",
    "          f\"Calificación estimada: {prediction.est:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
